jobs:
  job_common:
    jvm_opts:
      - -XX:+DisableExplicitGC
      - -XX:+HeapDumpOnOutOfMemoryError
      - -XX:+PrintGCApplicationStoppedTime
      - -XX:+PrintGCDateStamps
      - -XX:+PrintGCDetails
      - -XX:+PrintHeapAtGC
      - -XX:+UseConcMarkSweepGC
      - -XX:+UseMembar
      - -XX:CMSInitiatingOccupancyFraction=80
      - -XX:HeapDumpPath={{.PkgStdoutDir}}
      - -Xloggc:{{.PkgStdoutDir}}/gc.log
      - -Xmx1024m
      - -verbose:gc
    jvm_properties:
      - java.net.preferIPv4Stack=true
      - java.library.path={{.PkgRootDir}}/lib/native
      - hadoop.home.dir={{.PkgRootDir}}
      - hadoop.id.str=hadoop
      - hadoop.log.dir={{.PkgLogDir}}
      - hadoop.policy.file=hadoop-policy.xml
      - hadoop.root.logger=INFO,RFA
      - hadoop.security.logger=INFO,RFAS
      - hdfs.audit.logger=INFO,NullAppender
    config:
      core-site.xml:
        - fs.defaultFS=hdfs://%{cluster.name}
        - io.file.buffer.size=131072
        - hadoop.proxyuser.openinx.groups=*
        - hadoop.proxyuser.openinx.hosts=*
      hdfs-site.xml:
        - dfs.blocksize=268435456
        - dfs.replication=1
        # JournalNode
        - dfs.journalnode.rpc-address=0.0.0.0:%{journalnode.x.base_port}
        - dfs.journalnode.http-address=0.0.0.0:%{journalnode.x.base_port+1}
        - dfs.journalnode.edits.dir={{.PkgDataDir}}
        # NameNode
        - dfs.namenode.name.dir={{.PkgDataDir}}
        # NameNode HA
        - dfs.nameservices=%{cluster.name}
        - dfs.ha.namenodes.%{cluster.name}=nn0,nn1
        - dfs.namenode.rpc-address.%{cluster.name}.nn0=%{namenode.0.host}:%{namenode.0.base_port}
        - dfs.namenode.rpc-address.%{cluster.name}.nn1=%{namenode.1.host}:%{namenode.1.base_port}
        - dfs.namenode.http-address.%{cluster.name}.nn0=%{namenode.0.host}:%{namenode.0.base_port+1}
        - dfs.namenode.http-address.%{cluster.name}.nn1=%{namenode.1.host}:%{namenode.1.base_port+1}
        - dfs.client.failover.proxy.provider.mycluster=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
        - dfs.namenode.shared.edits.dir=qjournal://%{journalnode.server_list}/test-hdfs
        - dfs.ha.fencing.methods=shell(/bin/true)
        - dfs.ha.automatic-failover.enabled=true
        - dfs.journalnode.edits.dir={{.PkgDataDir}}
        - ha.zookeeper.quorum=%{dependencies.0.zkServer.server_list}
      log4j.properties:
        file: {{.ConfRootDir}}/hdfs/common/log4j.properties
      capacity-scheduler.xml:
        - yarn.scheduler.capacity.maximum-applications=10000
        - yarn.scheduler.capacity.maximum-am-resource-percent=0.1
        - yarn.scheduler.capacity.resource-calculator=org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator
        - yarn.scheduler.capacity.root.queues=default
        - yarn.scheduler.capacity.root.default.capacity=100
        - yarn.scheduler.capacity.root.default.user-limit-factor=1
        - yarn.scheduler.capacity.root.default.maximum-capacity=100
        - yarn.scheduler.capacity.root.default.state=RUNNING
        - yarn.scheduler.capacity.root.default.acl_submit_applications=*
        - yarn.scheduler.capacity.root.default.acl_administer_queue=*
        - yarn.scheduler.capacity.node-locality-delay=40
        - yarn.scheduler.capacity.queue-mappings=
        - yarn.scheduler.capacity.queue-mappings-override.enable=false
    classpath:
      - {{.PkgConfDir}}
      - {{.PkgRootDir}}/contrib/capacity-scheduler/*.jar
      - {{.PkgRootDir}}/lib/native/*
      - {{.PkgRootDir}}/share/hadoop/common/*
      - {{.PkgRootDir}}/share/hadoop/common/lib/*
      - {{.PkgRootDir}}/share/hadoop/hdfs
      - {{.PkgRootDir}}/share/hadoop/hdfs/*
      - {{.PkgRootDir}}/share/hadoop/hdfs/lib/*
      - {{.PkgRootDir}}/share/hadoop/mapreduce/*
      - {{.PkgRootDir}}/share/hadoop/mapreduce/lib/*
      - {{.PkgRootDir}}/share/hadoop/yarn/*
      - {{.PkgRootDir}}/share/hadoop/yarn/lib/*
  namenode:
    super_job: job_common
    jvm_properties:
      - hadoop.log.file=namenode.log
    main_entry:
      java_class: org.apache.hadoop.hdfs.server.namenode.NameNode
  datanode:
    super_job: job_common
    jvm_properties:
      - hadoop.log.file=datanode.log
    config:
      hdfs-site.xml:
        # DataNode
        - dfs.datanode.data.dir={{.PkgDataDir}}
        - dfs.datanode.address=0.0.0.0:%{datanode.x.base_port}
        - dfs.datanode.http.address=0.0.0.0:%{datanode.x.base_port+1}
        - dfs.datanode.ipc.address=0.0.0.0:%{datanode.x.base_port+2}
    main_entry:
      java_class: org.apache.hadoop.hdfs.server.datanode.DataNode
  journalnode:
    super_job: job_common
    jvm_properties:
      - hadoop.log.file=journalnode.log
    main_entry:
      java_class: org.apache.hadoop.hdfs.qjournal.server.JournalNode
  zkfc:
    super_job: job_common
    jvm_properties:
      - hadoop.log.file=zkfc.log
    main_entry:
      java_class: org.apache.hadoop.hdfs.tools.DFSZKFailoverController
  shell:
    super_job: job_common
    main_entry:
      java_class: org.apache.hadoop.fs.FsShell
